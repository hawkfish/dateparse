The first algorithm is a Minimum Descriptive Length~\cite{Rissanen:1978} approach derived from the domain system presented in the Potter's Wheel system of Raman \textit{et al.}~\cite{Raman:2001}. We describe a number of extensions to their structure extraction system to support more complex redundancy, non-English locales, improved performance and date-specific pruning.

\subsection{Domains}
Potter's Wheel presents an algorithm for deriving a common structure for a set of strings by breaking each string down into a sequence of \textit{domains}. A domain is a set of strings, with a few optional statistical properties. They are defined by an interface that includes:
\begin{itemize}
\item A required inclusion function to test for membership in the domain (\functionname{match}). 
For example, the domain \functionname{<Digits>} would return true for the string \functionname{"123"} 
and false for the string \functionname{"abc"}.
\item An optional function to compute the number of values in the domain with a given length (\functionname{cardinality}).
For example, the domain for three letter month names (\functionname{MMM}) would return $12$ for the length $3$ and $0$ otherwise.
\item An optional function to update statistics for the domain based on a given value (\functionname{updateStatistics}).
For example, the domain \functionname{MMM} might keep a histogram of how frequently each month was encountered.
\item An optional function to prevent consideration of a domain that is redundant (\functionname{isRedundantAfter}).
For example, the domain \functionname{<Digits>} would be redundant after itself 
(\ie there is no difference between a single run of digits and two adjacent runs.)
\end{itemize}

In our approach, we implement all of these functions, but with significant changes to the last one to increase level of pruning applied during
domain enumeration. These changes are described below.

With this interface, we can now define a set of domains for each date part that we wish to be able to parse. These are mostly straightforward enumerations and numeric ranges, each tagged with the ICU format code. Since the ICU parser is flexible about parsing single or double-digit formats, we use double-digit formats, but accept one or two digits. One important exception to this rule is for years, which are fixed width fields ($2$ or $4$).

We also included some enumerated domains for handling constant strings and some simple regular expression domains for delimiters such as spaces, punctuation or alphabetic characters. We found that the inclusion of arbitrary numeric domains caused the run time to grow exponentially as the number of possible matches could not be pruned intelligently. This restriction extends to domains that can contain arbitrary digit sequences (such as any). Because of this restriction, the algorithm cannot extract non-date numeric fields.

\subsection{Redundancy Extensions}
A difficulty in using this kind of structure extraction is that the algorithm for enumerating structures is exponential in the number of domains. This is especially true in the date format problem because there are identical domains (\eg months and meridian hours), nearly identical domains (\eg days and hours) and there are often no field delimiters (\eg \texttt{2012Mar06134427}). To handle this, we have extended the existing pruning API with two other sets of domain identifiers:
\begin{itemize}
\item A set of \textit{prunable} identifiers, which are not allowed to precede the domain. For example, once we have a month field, no other month fields should be generated. Each month domain therefore lists all the month domains in its prunable set.
\item A set of \textit{context} identifiers, one of which must have been previously generated before the domain is considered. For example, a meridian domain can only be generated once an hour field has been found, but there may be other intervening fields.
\end{itemize}

\subsection{Performance}
Structure enumeration is computationally expensive, so we added a number of enhancements to the original domain extraction algorithm to keep the run time low enough for interactivity.

\subsubsection{Domain Characteristics}
Date domains typically have small widths, so we found it advantageous to provide the shortest and longest match sizes for use in structure enumeration and matching. For example, months have between $1$ and $2$ digits, so there is no need to test matches whose length is outside this range.

Date domains are also often uniform in that adding more characters to a mismatch will not help. For example, a $2$-digit day domain that does not match a $1$-letter substring will not be able to generate a match by adding more characters.

\subsubsection{Parallel Evaluation}
We have also identified two opportunities for parallel computation during structure enumeration.

The first computationally expensive operation is the enumeration of structures for a given sample. To parallelize this step, each thread is given a subset of the samples and independently produces a set of candidate domains. When all threads have completed, the duplicates are removed to produce a single list of candidate structures.

The second computationally expensive operation is the evaluation of each generated structure over the entire sample set. This includes computation of the MDL, recording of domain statistics and parameterizing the structure. These tasks are simple to parallelize, because there are no overlaps between the data for each structure.

\subsection{Unparameterization}
Domain parameterization is an important part of generating compact representations via MDL, but it creates problems for date recognition. For example, if a set of dates contains a constant month string (\eg all values are in September) it is important to keep track of the month name domain. Consequently, when we parameterize a constant generic \texttt{<Word>} domain, we tag it with the date part domain that it matches (if any). We then need to apply an additional pruning step to remove any structures that also found an equivalent domain (\eg two-digit month). These rules are equivalent to the context-based redundancy rules above, but have to be applied again after parameterization of generics.

\subsection{Global Pruning}
The pruning rules used for the structure extraction reduce the search space dramatically, but they are also contextual and can only look backwards. The domains also contain a fair amount of ambiguity that requires the application of domain knowledge. We therefore found it necessary to add some post-generation global pruning rules:
\begin{itemize}
\item The set of date parts cannot contain place value gaps (\eg structures that have year and day without month are removed.)
\item Similarly, the set of time parts cannot contain place value gaps and must also be in place value order (times are never written in orders such as \texttt{mhs}.)
\item The existence of time parts cannot make dates incomplete (\eg patterns like year-day-hour are removed.)
\item Two digit years require special handling. In particular, they cannot appear adjacent to a two-digit field if the structure contains punctuation. (This can come up in some small early 21st century year domains where a two-digit year can masquerade as almost any numeric field \eg $08$ for $2008$.)
\end{itemize}

These global pruning rules are simple and intuitive, but are essential to further reducing the search space.

\subsection{Locale Sensitivity}
Providing an acceptable international user experience requires correctly handling the locale of the column text. Accordingly, at the start of the structure extraction we use the column locale to create a set of domains containing locale-sensitive strings such as month names. We also use the locale to map these strings to upper- and lower-case in addition to the ICU mixed case strings. This enables us to accurately compute MDL statistics without having to map the candidate strings at runtime (which would be slow).

Knowing the locale of a string is not always helpful. In our data set, we found numerous cases where the locale was specified (\eg Sweden) but the data was actually in English. Accordingly, we test both locales and rank the combined results. 

We have built this system for the Gregorian calendar only as our visualization system does not support non-Gregorian calendars and we have little evidence of other calendars (\eg Hebrew, Islamic Civil) being used for analytics. ICU supports non-Gregorian calendars and we expect the algorithm could be extended to them as well if needed.

\subsection{Ranking}
MDL structure analysis naturally produces a ranked list of format candidates, but we have found that a number of other properties of the formats should be preferred over simple compactness:
\begin{itemize}
\item Because we have a set of samples, we can apply the candidate format to the strings to see how well it performs. Formats with fewer parse errors are preferred.
\item Date parts can be considered a place-value system, so we prefer ``more significant'' components (\eg month-day-year over hour-minute-second).
\item If two formats from different locales give the same results, prefer the original column locale. The sample set may have missed an example where this could be important. For example, the month name for "September" is the same in both German and English, but ``October" and ``Oktober" are different.
\item If the format has an ambiguous date order (\eg all days are less than 12), then prefer the default date order of the locale. Again, the sample set may have missed a counterexample, so this is the best option.
\item Once these semantic preferences have been considered, we then prefer the more compact (MDL) representation.
\end{itemize}

The output of the algorithm is now an ordered list of formats and associated locales. These can then be used to drive a user interface that allows the user to choose between the possibilities or the top-ranking format can simply be used automatically.
