The main characteristic of formal grammar is the presence of production rules that are used to rewrite an expression, and a starting symbol which is the origin of all forms of a given expression. In the production rules, we find non-terminals and terminals, with parsed expressions containing only terminals. A formal grammar is considered ``context free'' when its production rules can be applied regardless of the context of a nonterminal. This has several advantages. First, it allows for modular syntax definitions, which simplifies grammar development and enables reuse. Second, it grants total freedom
in structuring a grammar to best fit its intended use. The power of such a grammar is its expressiveness, and is not constrained by pattern rigidities that are prevalent with regular expressions and text processing~\cite{Grune:1990}.


\subsection{Context-Free Grammar}
ICU date formats are well defined both structurally and semantically, and can be defined by a context-free grammar (CFG). A CFG consists of a set of non-terminal symbols $X$, a set of terminal symbols $\beta$, a start non-terminal symbol $S \in X$ from which the grammar generates the strings, and a set of production rules $\tau$, with each rule of the form $X \rightarrow \beta$~\cite{Hopcroft:1990}. 

While there are several functionally equivalent notations for representing CFG, we use the \textit{Backus-Naur Form} (BNF) for defining the grammar rules for \dateparse formats. In particular, we use the \textit{Extended Backus-Naur Form} (EBNF) as the notation is more compact and readable for frequently used constructions~\cite{Grune:1990}. We use a Cocke-Younger-Kasami (CYK) parser for syntactically parsing a column of date-time strings based on the grammar~\cite{Cocke:1969,Younger67,Kasami:1965}. The parser is a dynamic programming algorithm that identifies the highest probable syntactic parse trees, given the grammar and an input date-time string.


We define a grammar for identifying date-time strings based on other EBNF based date-time formats as a reference  [cite] . The definition of the grammar is found below:

\begin{grammar}
<TimeGrammar> ::= <Hours> ':' <Minutes> ':' <Seconds> <TimeZone> <AMPM> (for 12-hour formats)

<DateGrammar> ::= <BigEndianDate> 
				\alt <MiddleEndianDate> 
				\alt <LittleEndianDate>;

<DateTimeGrammar>  ::= <DateGrammar> 
					\alt <TimeGrammar>;
					

<BigEndianDate> ::= <Year> <Month>  <Day> ;

<MiddleEndianDate> ::= <Month> <Day> <Year>;

<LittleEndianDate> ::= <Day> <Month> <Year>;

<Year> ::= <TwoYear> | <FourYear>;

<QuarterYear> := <Quarter> <Year>;

<Day>     ::= dd (where $dd \in [01-31]$, depending on month/year);

<Month> := <MonthWord> | <MonthNumber>;

<MonthWord> := `January' | `February' | `March' | `April' | `May' | `June' | `July' | `August' | `September' | `October' | `November' | `December';

<MonthNumber> := dd (where $dd \in [01-12]$);

<DayOfWeek> ::= `Monday' | `Tuesday' | `Wednesday' | `Thursday' | `Friday' | `Saturday' | `Sunday';

<Hour> ::= <TwelveHour> | <TwentyFourHour>;

<Quarter> ::= `Quarter' dd (where $dd \in [01 - 04]$);

<TwoYear> ::= dd;

<FourYear> := dddd;

<TwelveHour> ::= dd (where $dd \in [00-12]$);

<TwentyFourHour> ::= dd (where $dd \in [00 -23]$);

<AMPM> := `a.m.' | `p.m.';

<d> ::= `0' | `1' | `2' | `3' | `4' | `5' | `6' | `7' | `8' | `9';

\end{grammar}

The parsed output is then converted to the ICU Date Format Language described in Section 3.2 in order to effectively cross-validate results from the MDL algorithm.

\subsection{Grammar Variants and Constraints}

A classical CFG performs poorly on inflected expressions resulting from misapplication of morphological inflection and syntactic rules. Large numbers of non-terminals are necessary for representation of all variations of such features, and the parser output would consist of many parse trees. We extend the date-time grammar by adding morpho-syntactic variants. These variants allows the parser to correct for syntactic errors (\textit{e.g.} usage of whitespace characters and punctuation marks), capitalization errors and abbreviations (\textit{e.g.} `Mon' for `Monday'). We use several various external corpora for such corrections~\cite{nltkcorpora}.

The date-time grammar also includes a large number of syntactically correct but semantically invalid
date-time expressions. While we have added range restrictions to symbols such as \texttt{Hour} (1--12 for 12-hour format and 1--24 for 24-hour format), \texttt{Days} (1--7), \texttt{Month} (1--12), there are special cases that need to be accounted for. For example, there is no  `November 31, 2015', `February 29, 2013', or `Sunday, May  5, 1965'. November only as 30 days in any year; 2013 was not a leap year; and May 5, 1965 was a Wednesday.

While custom production rules can be added to the existing grammar to exclude such expressions, this approach is not optimal as it leads to a rather large grammar that needs to account for every single semantically valid date-time sequence of terminal symbols. Rather, we modify the existing grammar with the following additional constraints to the \texttt{Day} terminal symbol for excluding such expressions:

\begin{itemize}
\item Restriction on the distribution of $30$ and $31$:
Months usually alternate between lengths of 30 and 31 days. We can use x mod 2 to get an alternating pattern of 1 and 0, then just add our constant base number of days:

\begin{equation}
\texttt{Day} = 30 + x \Mod 2
\end{equation}

where $x \in [1..12]$ for each of the $12$ months.

Except February, Equation $1$ addresses January and March through July. After July, the pattern should skip one, and the rest of the months should follow the alternating pattern inversely.

To obtain an inverse pattern of alternating 0 and 1, we add 1 to the dividend:

\begin{equation}
\texttt{Day} = 30 + (x + 1) \Mod 2
\end{equation}

In Equation $2$ the number of days for August through December is correct ($x \in [8..12]$), but not for the remaining months. We hence introduce a bit-masking function so that the equation is equal to 1 over the desired domain and 0 otherwise. Multiplying a term by this expression will result in the term being cancelled out outside its domain. To mask the latter piece of our function, we need an expression equal to $1$ where $8 \le x\le 12$. Floor division by 8 works well, since $x < 16$.

Now if we substitute this expression in the x + 1 dividend of Equation $2$, we can invert the pattern using our mask:

\begin{equation}
\texttt{Day} = 30 + (x + \floor{\frac{x}{8}}) \Mod 2
\end{equation}


\item LeapYear Restriction for February:
While the above restriction applies to all months barring February, we also apply a constraint to the number of days for February, based on whether the year is leap year or not. For this, we define a new symbol in the grammar called \texttt{LeapYear}. If an expression containing the month `February' or any such variant (\textit{e.g.} `Feb', `2') with the day `29' and a year, would need to resolve the \texttt{Year} symbol to be a \texttt{LeapYear}, defined as:

\begin{equation}
\texttt{Year} \Mod 4 == 0
\end{equation}

\end{itemize}

Equations $3$ and $4$ are now added as constraints to the \texttt{Days} symbol in the grammar.

\subsection{Probabilistic Context-Free Grammar}

Pattern-recognition problems such as parsing date and time formats initiate from observations generated by some structured stochastic process. In other words, even if the initial higher-level production rule of the grammar is known (i.e. date, time or date-time), there could be several directions that the parser resolves to. For example, in a date string \texttt{5/6/2015}, the pattern could either be \texttt{M/d/yyyy} or \texttt{d/M/yyyy}. 

In the context of CFGs, probabilities have been used to define a probability distribution over a set of parse trees defined by the CFG, and are a useful method for modeling such ambiguity~\cite{Collins:2003,Manning:1999}. The resulting formalism called Probabilistic Context-Free Grammar (PCFG), extends the CFG by assigning probabilities to the production rules of the grammar. During the process of parsing the date-time pattern, the probabilities are used as a filtering mechanism to rank the pattern(s) that a given string resolves to in the grammar. 

The production rules are of the form $ X \rightarrow \beta$ where $X$ is the left-hand side of the rule, while $\beta$ is the right-hand side.

Given a CFG grammar $G$, we define  $\tau_G(s)$ as the set of all possible parse trees for input date-time string $s$.  For any $X \rightarrow \beta \in \tau(G)$, the probability $p(X \rightarrow \beta) \ge 1$. In addition, $\sum_{(X \rightarrow \beta) \in \tau_{G}} = 1$. The parser then chooses the parse tree with the maximum probability, \textit{i.e.} given a date-time string $s$, as input, we determine the highest scoring parse tree $X \rightarrow \beta$ as  $max_{(X \rightarrow \beta) \in \tau(s)} p(X \rightarrow \beta)$.

In order to bootstrap the probability weights in the \texttt{PCFG}, we manually set initial probability weights to various rules in the grammar as follows:
\begin{itemize}
\item Similar to the MDL ranking properties described in Section 3.7, rules involving the non-terminal \texttt{DateGrammar} on the left-side of the rule, are assigned probability weights higher than those rules with the non-terminal being \texttt{TimeGrammar}. In practice, assigning $p(\texttt{DateGrammar} \rightarrow \beta_{1}) = 0.9$ and $p(\texttt{TimeGrammar} \rightarrow \beta_{2}) = 0.7$ leads to optimal ranking of the parse trees.

\item For the non-terminal symbol \texttt{Day}, the constraint specified is a range between 01 and 28-31, depending on month/year. However, for \texttt{dd} $> 12$, $p(\texttt{Day} \rightarrow d d) = 1.0$, and $0.5$ otherwise. This assignment helps disambiguate days from months.


\end{itemize}

s

\subsection{Supervised Learning}

Having defined initial probabilistic weights to the \texttt{PCFG}, we employ supervised learning with a known training set of date time formats to estimate the rule probabilities.  The training corpus is a set of files obtained from an online data analysis corpus, described in Section 5.

The occurrence frequencies of the rules in the correct
(disambiguated) parse trees can be determined from the corpus using the maximum-likelihood parameter estimates.
\begin{equation}
p(X \rightarrow \beta) = \frac{Count(X \rightarrow \beta)}{Count(X)}
\end{equation}

where $Count(X \rightarrow \beta)$ is the number of times that the rule $X \rightarrow \beta$ is seen in
the parse trees $\tau$, and $Count(X)$ is the number of times the non-terminal $X$ is seen in $\tau$. These frequencies can then be normalized into rule probabilities. This method produces accurate probability estimates when trained on a sufficiently large corpus of disambiguated parse trees. 

\subsection{Context Extensions to the Grammar}

The grammar parser's success is often limited by data ambiguity, and incomplete expressions. For example an input string `3/7/2005' could either be interpreted as as the 7th day of March or the 3rd day of July depending on the date format used. While a single entry cannot resolve the parser's ambiguity, a column of data provides more context for determining the dominant pattern prevalent in the dataset. 

The CKY parser is run over the same set of samples from each validation file to generate a probabilistic distribution of possible parse trees corresponding to a set of date-time patterns. As a second pass of the parser, starting from the most probable parse tree, each parse tree is then applied to the entire file's columnar data to determine the dominant pattern. For example, another entry in the same dataset could be `25/3/2007' thus increasing the probability that the format is \texttt{dd/mm/yyyy} as opposed to \texttt{mm/dd/yyyy}.

In addition to the  columnar data context, the locale is are also used to help maximize the probabilistic likelihood of a particular date-time pattern. The locale of the data column is used to retrieve the corresponding corpora for helping with the parsing and for correcting syntactical errors. However, similar to the locale sensitivity issue described in Section 4.6, the locale and the data may not always match. The dominant pattern is simply computed over the data column based on the ranked set of parse tree results.
