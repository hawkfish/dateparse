In recent years, there has been a growth of interest in data visualization technologies for human-assisted data analysis using systems such as \cite{Polaris,Qlik,Spotfire}. While computers can provide high-speed and high-volume data processing, humans have domain knowledge and the ability to process data in parallel, notably by using our visual systems. Most importantly, humans provide the definition of what is valuable in an analysis. Accordingly, human/computer analytic systems are essential to extracting knowledge of value to humans and their societies from the large amounts of data being generated today.

\subsection{Interactivity}
Visualization systems are most effective when they are interactive, thereby allowing a user to explore data and connect it to their domain knowledge and sense of what is important without breaking cognitive flow. In recent years, a number of such systems have been developed, both by the academic community and by the commercial sector. Exploration of data consists not only in creating visual displays, but also in creating and modifying domain-specific computations. Consequently, most data visualization systems include facilities for defining such calculations as part of the data model being analyzed. The most effective systems allow users to define these calculations as part of the analytic interaction, which permits the user to stay in the flow of analysis \cite{Morton}.

During the analytic process, a user may discover that parts of the data are not yet suitable for analysis. Solutions to this problem are often provided by data preparation tools external to the visual analysis environment, which requires the user to break their analytic flow, launch another tool and reprocess their data before returning to their analysis. If the user does not own this process (\textit{e.g.} it is the responsibility of another department), then there can be significant delays (including ``never.'') More subtly, the result of updated external processing may not be compatible with the user's existing work, which can lead to more time lost reconciling the new data model with the existing analysis.

From the user's perspective, the boundary between preparation and analysis is not nearly so clean cut. Bad data is often discovered using visual analysis techniques (e.g. histograms or scatter plots) and it is most natural for the user to ``clean what she sees'' instead of switching to a second tool. This leads to an ``adaptive'' process whereby users will prefer leveraging existing tools in the analytics environment (no matter how well suited to the task) over switching to another application. Thus a well-designed interactive visual analysis environment will provide tools that enable users to perform such cleaning tasks as interactively as possible.

\subsection{The Tableau Ecosystem}
In this paper, we shall be looking at an example of this problem in the context of the Tableau system. Tableau is a commercial visual analysis environment derived from the Polaris \cite{Polaris} system developed at Stanford. In addition to an interactive desktop application for creating data visualizations, the Tableau ecosystem also includes servers for publishing and sharing interactive visualizations. These servers can be privately operated by individual customers or hosted in the cloud. 

\subsubsection{Tableau Public}
Of particular relevance in the present work is a free version of the server environment called Tableau Public \cite{Public}. Tableau Public (or ``Public'') allows authors to publish data visualizations that can be shared with the general public. The published data sets are limited to 100K rows and must be stored using the Tableau Data Engine (or ``TDE'')\cite{TDE1,TDE2}, but are otherwise free to use the full analytic capabilities of the Tableau system. In particular, the TDE provides native support for all analytic calculations defined by the authoring and publishing components. Moreover, visualizations published to Public are uploaded as complete Tableau workbooks, including the data model developed as part of the analysis. This makes it a rich source of data on the analytic habits and frustrations of Tableau users.

\subsubsection{Functions}
While the existing standards for query languages are helpful for defining a core set of row-level functions, there are many useful functions beyond the standards that are only explicitly supported by a subset of RDBMSes, often with different names. And even when a database does not provide an implementation of a particular function, it is usually possible to generate it as an inline combination of existing functionality. Tableau's calculation language is designed to be a lingua franca that tames this Babel of dialects by providing a common syntax for as many of these functions as possible.

\subsection{DATEPARSE}
Among the recent additions to the Tableau function library is a function called \texttt{DATEPARSE}, the usability of which is the focus of the work in this paper.

\subsubsection{	Scalar Dates}
The SQL-99 standard defines three temporal scalar types: \texttt{DATE}, \texttt{TIMESTAMP} and \texttt{TIME}. They are typically implemented as fixed-point types containing an offset from some epoch (\textit{e.g.} Julian Days.) This makes them compact to store and allows some temporal operations to be implemented very efficiently using arithmetic. Thus from the RDBMS perspective, representing dates in scalar form provides numerous benefits for users, both in terms of available operations and query performance.

Tableau models the first two of these types as ``Date'' and ``Date \& Time''; the third (pure time) is folded into Date \& Time by appending it to a fixed date of \texttt{1899-12-30}. From the analytic perspective, date types are dimensional (\textit{i.e.} independent variables) and can be used as either categorical (simply ordered) or quantitative (ordered with a distance metric) fields.

Categorical dates have a natural hierarchy associated with them generated by calendar binning. The visualization in Figure I1 shows an example of a bar chart employing binned categorical dates in a year/quarter hierarchy.

\subsubsection{Parsing}
One of the oldest data preparation problems observed in Tableau is the parsing of date scalars so that users can perform these kinds of analyses. Training materials from the early days of the company included examples of how to convert columns of integers in the form \texttt{yyyyMMdd} to date scalars. The documented solution at the time was to convert the integer to a string and perform some locale-dependent string operations before casting the string back to a date. Unfortunately, this approach had a number of problems:
\begin{itemize}
\item String operations are notoriously slow compared to scalar operations (typically 10-100x slower)
\item Default parsing of date formats is locale-dependent, and may not work when the workbook is shared across an international organization (\textit{e.g.} between the US and European offices)
\item The expression code was hard to understand and maintain because it used a verbose, general-purpose string-handling syntax instead of a domain language.
\end{itemize}

This is but a single format. Our studies of the workbooks on Public suggest that there are hundreds of distinct temporal date formats in user data sets. Some are common, but others can be quite idiosyncratic. Table 1 shows a selection of unusual date formats found on Public.

% INSERT TABLE HERE

Our first solution to this problem was to add a new function to the Tableau calculation language called \texttt{DATEPARSE}. This function would take a string column and convert it to a datetime using a special purpose domain language for describing dates. Such functions exist in a number of databases (\textit{e.g.} Oracle, Postgres and MySQL) and adding it to the TDE was straightforward, so it was a natural addition to the function library.

We chose the date parsing syntax defined by the International Components for Unicode (or ICU) project \cite{icu} for the common domain language because it mirrored what was available in the Tableau code base, and translating this syntax into the formats used by various database vendors (\textit{e.g.} MySQL, Oracle, Postgres) was relatively painless. There are a few patches of non-overlapping functionality, but they tend to be obscure and we provide warnings when functionality is missing.

\subsubsection{	Usability}
After shipping this new function to our user base, we investigated how it was being applied in the field. We examined a few months of workbooks that had been uploaded to Public after the release to see if and how it was being used. We found that although there were a number of new calculations using \texttt{DATEPARSE}, the error rate for the domain language syntax was about 15\%. \texttt{DATEPARSE} was capable of solving the problem, and some users were able to discover it, but the syntax did not appear to be easy to use reliably.

\subsection{Automated Format Extraction}
One solution might have been to design a graphical environment that enabled users to construct valid patterns. This would have involved a substantial development investment with no guarantee that the result would be correct if the user misunderstood the environment. Instead, we have developed two algorithms for automating the derivation of the format string, each of which uses a different machine learning technique. Both algorithms result in over 90\% parsing accuracy -- and 100\% syntactic correctness because they are machine generated.

% vidya: this probably should be its own section
% hawkfish: Not sure it is big enough, but I can see the logic.

\subsection{Related Work}
Data preparation has been a known analytic bottleneck since at least the description of the Potter's Wheel system \cite{PottersWheel}. Since then, several other interactive data preparation systems have been proposed \cite{Wrangler,Refine}. While effective, these systems all make assumptions about possible date formats (\textit{e.g.} the domain system in \cite{PottersWheel}), which we suggest are too restrictive for real world data.

The Minimum Descriptive Length technique was first proposed in \cite{mdl}.  We have built on the version described in \cite{PottersWheel}.

Related work on parsing languages, outlier detection.

\subsection{Overview}
The rest of this paper is organized as follows: The next section introduces the parameters of the problem space. The following two sections describe the two different algorithms, one using Minimum Descriptive Length and the other using Natural Language Processing. In section 5, we evaluate the algorithms on a corpus of 30K columns, both by sampling the outputs manually and then by using the algorithms to validate each other. We then discuss future work in section 6 and conclude in section 7.
