In recent years, there has been a growth of interest in data visualization technologies for human-assisted data analysis using systems such as [1,10,11]. While computers can provide high-speed and high-volume data processing, humans have domain knowledge and the ability to process data in parallel, notably by using our visual systems. Most importantly, humans provide the definition of what is valuable in an analysis. Accordingly, human/computer analytic systems are essential to extracting knowledge of value to humans and their societies from the large amounts of data being generated today.

\subsection{Interactivity}
Visualization systems are most effective when they are interactive, thereby allowing a user to explore data and connect it to their domain knowledge and sense of what is important without breaking cognitive flow. In recent years, a number of such systems have been developed, both by the academic community and by the commercial sector. Exploration of data consists not only in creating visual displays, but also in creating and modifying domain-specific computations. Consequently, most data visualization systems include facilities for defining such calculations as part of the data model being analyzed. The most effective systems allow users to define these calculations as part of the analytic interaction, which permits the user to stay in the flow of analysis [9].

During the analytic process, a user may discover that parts of the data are not yet suitable for analysis. Solutions to this problem are often provided by data preparation tools external to the visual analysis environment, which requires the user to break their analytic flow, launch another tool and reprocess their data before returning to their analysis. If the user does not own this process (e.g. it is the responsibility of another department), then there can be significant delays (including ``never.") More subtly, the result of updated external processing may not be compatible with the user's existing work, which can lead to more time lost reconciling the new data model with the existing analysis.

From the user's perspective, the boundary between preparation and analysis is not nearly so clean cut. Bad data is often discovered using visual analysis techniques (e.g. histograms or scatter plots) and it is most natural for the user to ``clean what she sees" instead of switching to a second tool. This leads to an ``adaptive" process whereby users will prefer leveraging existing tools in the analytics environment (no matter how well suited to the task) over switching to another application. Thus a well-designed interactive visual analysis environment will provide tools that enable users to perform such cleaning tasks as interactively as possible.

\subsection{The Tableau Ecosystem}
In this paper, we shall be looking at an example of this problem in the context of the Tableau system. Tableau is a commercial visual analysis environment derived from the Polaris [1] system developed at Stanford. In addition to an interactive desktop application for creating data visualizations, the Tableau ecosystem also includes servers for publishing and sharing interactive visualizations. These servers can be privately operated by individual customers or hosted in the cloud. 

\subsubsection{Tableau Public}
Of particular relevance in the present work is a free version of the server environment called Tableau Public [12]. Tableau Public (or ``Public") allows authors to publish data visualizations that can be shared with the general public. The published data sets are limited to 100K rows and must be stored using the Tableau Data Engine (or ``TDE") [2, 3], but are otherwise free to use the full analytic capabilities of the Tableau system. In particular, the TDE provides native support for all analytic calculations defined by the authoring and publishing components. Moreover, visualizations published to Public are uploaded as complete Tableau workbooks, including the data model developed as part of the analysis. This makes it a rich source of data on the analytic habits and frustrations of Tableau users.

\subsubsection{Functions}
While the existing standards for query languages are helpful for defining a core set of row-level functions, there are many useful functions beyond the standards that are only explicitly supported by a subset of RDBMSes, often with different names. And even when a database does not provide an implementation of a particular function, it is usually possible to generate it as an inline combination of existing functionality. Tableau's calculation language is designed to be a lingua franca that tames this Babel of dialects by providing a common syntax for as many of these functions as possible.

\subsection{DATEPARSE}
Among the recent additions to the Tableau function library is a function called DATEPARSE, the usability of which is the focus of the work in this paper.

\subsubsection{	Scalar Dates}
The SQL-99 standard defines three temporal scalar types: DATE, TIMESTAMP and TIME. They are typically implemented as fixed-point types containing an offset from some epoch (\textit{e.g.} Julian Days.) This makes them compact to store and allows some temporal operations to be implemented very efficiently using arithmetic. Thus from the RDBMS perspective, representing dates in scalar form provides numerous benefits for users, both in terms of available operations and query performance.

Tableau models the first two of these types as ``Date" and ``Date \& Time"; the third (pure time) is folded into Date \& Time by appending it to a fixed date of 1899-12-30. From the analytic perspective, date types are dimensional (\textit{i.e.} independent variables) and can be used as either categorical (simply ordered) or quantitative (ordered with a distance metric) fields.
Categorical dates have a natural hierarchy associated with them generated by calendar binning. The visualization in Figure I1 shows an example of a bar chart employing binned categorical dates in a year/quarter hierarchy.
